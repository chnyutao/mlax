# Multi-Layer Perceptron

[![wandb badge](https://img.shields.io/badge/Weights_&_Biases-FFCC33?style=for-the-badge&logo=WeightsAndBiases&logoColor=black)](https://wandb.ai/chnyutao/mlax/runs/n2oh0ikv)

Multi-layer perceptron (MLP), also known as fully-connected networks, is one of the simplest forms of neural networks. It consists of stacked layers of linear transformations with non-linear activation functions (e.g. ReLU).

## Experiment results

Experiments are conducted on the MNIST[^1] dataset to predict the label (0-9) given an handwritten digit image. The final predictive accuracy on the test set was ~92.15%.

![plot](https://api.wandb.ai/files/chnyutao/mlax/n2oh0ikv/media/images/plot_9390_4dd8daf5a88a68de1ccc.png)

[^1] LeCun, Yann, et al. "Gradient-based learning applied to document recognition." _Proceedings of the IEEE_ 86.11 (1998): 2278-2324.
